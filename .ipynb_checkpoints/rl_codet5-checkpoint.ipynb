{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45f672e",
   "metadata": {},
   "source": [
    "### Define args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8126c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "parent_dir = '/home/grads/tsaisindhura/datasets/XLCoST/pair_data_tok_full/'\n",
    "dir_dict = {'javascript':'Javascript', 'java':'Java', 'c_sharp':'C#', 'php':'PHP', 'python':'Python', 'c':'C', 'cpp':'C++'}\n",
    "end_dict = {'javascript':'js', 'java':'java', 'c_sharp':'cs', 'php':'php', 'python':'py', 'c':'c', 'cpp':'cpp'}\n",
    "l1, l2 = 'php', 'python'\n",
    "data_dir = parent_dir + dir_dict[l1] + '-' + dir_dict[l2] + '/'\n",
    "template = data_dir+'train-XXX-YYY-tok.xxx,'+data_dir+'train-XXX-YYY-tok.yyy'\n",
    "template = template.replace('XXX', dir_dict[l1]).replace('YYY', dir_dict[l2])\n",
    "if not(os.path.exists(data_dir)):\n",
    "    data_dir = parent_dir + dir_dict[l2] + '-' + dir_dict[l1] + '/'\n",
    "    template = data_dir+'train-XXX-YYY-tok.xxx,'+data_dir+'train-XXX-YYY-tok.yyy'\n",
    "    template = template.replace('XXX', dir_dict[l2]).replace('YYY', dir_dict[l1])\n",
    "train_filename = template.replace('xxx', end_dict[l1]).replace('yyy', end_dict[l2])\n",
    "dev_filename = train_filename.replace('train', 'val')\n",
    "test_filename = train_filename.replace('train', 'test')\n",
    "baseline_output_dir = 'baselines/codet5/saved_models/'+l1+'-'+l2+'/'\n",
    "load_model_path = 'baselines/codet5/saved_models/'+l1+'-'+l2+'/checkpoint-best-bleu/pytorch_model.bin'\n",
    "output_dir = 'saved_models/codet5/saved_models/'+l1+'-'+l2\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train_filename = train_filename\n",
    "        self.dev_filename = dev_filename\n",
    "        self.test_filename = test_filename\n",
    "        self.baseline_output_dir = baseline_output_dir\n",
    "        self.load_model_path = load_model_path\n",
    "        self.max_source_length = 400\n",
    "        self.max_target_length = 400\n",
    "        self.train_batch_size = 16\n",
    "        self.train_epochs = 1000000\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.n_gpu = torch.cuda.device_count()\n",
    "        self.output_dir = output_dir\n",
    "        if not(os.path.exists(self.output_dir)):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "args = Args()\n",
    "print ('# gpu:', args.n_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f73f5",
   "metadata": {},
   "source": [
    "### Check #errors in generated codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd84ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.output [[0, 66], [0, 235], [0, 169], [0, 154], [0, 48], [1, 141], [0, 276], [0, 248], [0, 102], [0, 225], [0, 53], [0, 116], [0, 222], [0, 152], [0, 264], [0, 255], [0, 296], [0, 195], [0, 214], [0, 190], [0, 97], [0, 56], [0, 63], [0, 148], [0, 165], [0, 115], [0, 55], [1, 382], [0, 299], [0, 85], [0, 220], [0, 140], [0, 173], [0, 219], [0, 128], [0, 130], [0, 203], [0, 172], [0, 166], [0, 206], [0, 236], [0, 103], [0, 116], [1, 197], [0, 84], [0, 94], [0, 207], [0, 289], [0, 174], [0, 355], [0, 105], [0, 215], [1, 364], [0, 189], [0, 118], [0, 139], [0, 119], [0, 200], [0, 141], [0, 137], [0, 129], [0, 140], [0, 249], [0, 93], [0, 137], [0, 132], [0, 164], [0, 91], [0, 80], [0, 135], [3, 202], [0, 313], [0, 75], [0, 214], [0, 204], [0, 165], [1, 359], [1, 359], [0, 285], [0, 109], [0, 63], [0, 147], [0, 123], [0, 80], [0, 149], [0, 132], [0, 131], [0, 92], [0, 147], [0, 284], [0, 208], [0, 70], [0, 161], [0, 59], [0, 199], [0, 160], [0, 324], [0, 203], [0, 136], [0, 153], [0, 49], [0, 122], [0, 337], [0, 230], [0, 344], [0, 280], [0, 107], [0, 261], [0, 247], [0, 139], [0, 86], [0, 173], [0, 232], [0, 111], [0, 115], [0, 101], [0, 67], [0, 79], [0, 96], [0, 186], [0, 203], [0, 304], [0, 75], [0, 222], [0, 195], [0, 85], [0, 203], [0, 131], [0, 176], [0, 185], [0, 87], [0, 66], [0, 100], [0, 174], [0, 138], [1, 367], [0, 117], [1, 257], [0, 259], [0, 144], [0, 156], [0, 430], [0, 105], [0, 258], [0, 45], [0, 75], [0, 247], [0, 269], [0, 76], [0, 193], [0, 244], [0, 120], [0, 283], [0, 192], [0, 155], [0, 160], [0, 98], [0, 201], [0, 123], [0, 247], [0, 276], [0, 76], [0, 88], [0, 213], [2, 246], [0, 57], [0, 140], [0, 102], [0, 222], [0, 184], [0, 122], [0, 341], [0, 312], [0, 80], [0, 148], [0, 259], [0, 372], [0, 193], [0, 226], [0, 163], [0, 119], [0, 212], [0, 49], [0, 189], [0, 91], [0, 223], [0, 161], [0, 152], [0, 182], [0, 102], [0, 80], [0, 153], [0, 114], [0, 233], [0, 189], [0, 112], [0, 240], [0, 62], [1, 367], [0, 172], [0, 106], [0, 101], [0, 153], [1, 368], [0, 72], [0, 356], [0, 42], [0, 118], [0, 150], [0, 83], [0, 101], [0, 257], [0, 96], [0, 154], [0, 67], [0, 123], [0, 136], [0, 266], [0, 111], [0, 127], [0, 160], [0, 144], [0, 89], [0, 130], [0, 167], [0, 84], [3, 319], [0, 242], [0, 403], [0, 350]]\n",
      "test_0.output [[0, 176], [0, 117], [0, 111], [0, 215], [0, 197], [0, 162], [0, 332], [0, 363], [0, 120], [0, 197], [4, 249], [1, 339], [0, 134], [0, 180], [0, 144], [0, 299], [3, 207], [0, 114], [0, 189], [0, 177], [0, 260], [0, 73], [0, 288], [0, 271], [1, 427], [0, 399], [1, 396], [0, 130], [0, 165], [0, 178], [0, 182], [0, 242], [0, 352], [0, 300], [0, 190], [0, 289], [0, 173], [0, 173], [0, 232], [0, 141], [0, 334], [0, 123], [0, 241], [0, 116], [0, 211], [0, 114], [0, 188], [0, 83], [0, 158], [0, 234], [0, 147], [0, 162], [0, 130], [0, 64], [0, 91], [0, 62], [0, 35], [0, 112], [0, 56], [0, 87], [0, 87], [0, 109], [0, 97], [1, 303], [0, 64], [0, 64], [0, 61], [0, 203], [0, 154], [0, 122], [1, 251], [0, 257], [0, 94], [0, 209], [0, 65], [0, 196], [0, 299], [0, 171], [0, 132], [0, 58], [0, 114], [0, 251], [0, 149], [0, 202], [0, 306], [0, 49], [0, 140], [0, 91], [0, 275], [0, 198], [0, 231], [0, 397], [0, 51], [0, 256], [6, 250], [1, 350], [1, 342], [0, 117], [0, 191], [0, 165], [0, 105], [0, 167], [0, 222], [0, 280], [0, 205], [0, 181], [0, 87], [0, 220], [0, 103], [0, 123], [0, 141], [0, 114], [0, 78], [0, 48], [0, 77], [0, 83], [0, 89], [0, 129], [0, 86], [0, 116], [0, 335], [0, 236], [1, 324], [0, 45], [0, 98], [0, 395], [0, 141], [0, 141], [0, 125], [0, 143], [0, 54], [0, 99], [0, 143], [1, 70], [0, 125], [0, 110], [0, 164], [0, 292], [1, 438], [0, 135], [1, 392], [0, 144], [0, 135], [0, 96], [0, 152], [0, 131], [0, 49], [0, 135], [0, 109], [0, 161], [0, 204], [0, 74], [0, 156], [0, 74], [0, 110], [0, 98], [0, 87], [0, 108], [0, 75], [2, 216], [0, 182], [0, 176], [0, 97], [0, 147], [0, 133], [0, 135], [0, 204], [0, 93], [2, 141], [0, 80], [0, 59], [0, 67], [0, 104], [1, 144], [0, 66], [0, 225], [0, 191], [0, 130], [1, 294], [0, 293], [2, 247], [0, 122], [0, 82], [0, 341], [0, 287], [0, 197], [0, 71], [0, 130], [0, 97], [0, 218], [0, 146], [0, 129], [0, 90], [0, 109], [0, 118], [0, 93], [0, 164], [0, 191], [0, 110], [0, 172], [0, 83], [0, 122], [0, 54], [0, 92], [0, 95], [0, 78], [0, 42], [0, 166], [0, 143], [0, 164], [0, 245], [0, 216], [0, 300], [0, 137], [2, 72], [0, 230], [0, 109], [0, 106], [0, 132], [0, 179], [0, 100], [0, 143], [0, 219], [0, 185], [0, 146], [1, 301], [0, 140], [0, 194], [0, 133], [0, 141]]\n"
     ]
    }
   ],
   "source": [
    "from reward import remove_special_tokens, tree_sitter_full_compile\n",
    "\n",
    "def get_num_errors(filepath):\n",
    "    codes = open(filepath).readlines()\n",
    "    codes = [remove_special_tokens(code[:-1]) for code in codes]\n",
    "    num_errors = [tree_sitter_full_compile(code) for code in codes]\n",
    "    return num_errors\n",
    "\n",
    "for filename in ['dev.output', 'test_0.output']:\n",
    "    num_errors = get_num_errors(args.baseline_output_dir+filename)\n",
    "    print (filename, num_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10bc55d",
   "metadata": {},
   "source": [
    "### Functions to prepare base model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2df603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "\n",
    "# extract features\n",
    "class Example(object):\n",
    "    def __init__(self,\n",
    "                 idx,\n",
    "                 source,\n",
    "                 target,\n",
    "                 ):\n",
    "        self.idx = idx\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        \n",
    "def read_examples(filename, args):\n",
    "    examples=[]\n",
    "    assert len(filename.split(','))==2\n",
    "    src_filename = filename.split(',')[0]\n",
    "    trg_filename = filename.split(',')[1]\n",
    "    idx = 0\n",
    "    with open(src_filename) as f1,open(trg_filename) as f2:\n",
    "            for line1,line2 in zip(f1,f2):\n",
    "                line1=line1.strip().replace('▁', '_')\n",
    "                line2=line2.strip().replace('▁', '_')\n",
    "                examples.append(\n",
    "                Example(idx = idx, source=line1, target=line2))\n",
    "                idx+=1\n",
    "    return examples\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "                 target\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask   \n",
    "        self.target = target\n",
    "        \n",
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(examples):\n",
    "        #source\n",
    "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
    "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        padding_length = args.max_source_length - len(source_ids)\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_mask+=[0]*padding_length\n",
    " \n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-1]\n",
    "        target_tokens = target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[-100]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "                 example.target\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "def get_dataset(features):\n",
    "    all_source_ids = torch.tensor([f.source_ids for f in features], dtype=torch.long)\n",
    "    all_source_mask = torch.tensor([f.source_mask for f in features], dtype=torch.long)\n",
    "    all_target_ids = torch.tensor([f.target_ids for f in features], dtype=torch.long)\n",
    "    all_target_mask = torch.tensor([f.target_mask for f in features], dtype=torch.long)  \n",
    "    indices = torch.arange(len(features))\n",
    "    data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask,indices)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d6f41",
   "metadata": {},
   "source": [
    "### Prepare for PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a1fd2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl.codet5 import CodeT5HeadWithValueModel, respond_to_batch\n",
    "from transformers import RobertaTokenizer\n",
    "from trl.ppo import PPOTrainer\n",
    "import torch\n",
    "from reward import get_reward\n",
    "\n",
    "# get models\n",
    "model = CodeT5HeadWithValueModel()\n",
    "model.load_base_model(args.load_model_path)\n",
    "model.to(args.device)\n",
    "\n",
    "model_ref = CodeT5HeadWithValueModel()\n",
    "model_ref.load_base_model(args.load_model_path)\n",
    "model_ref.to(args.device)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-base\", do_lower_case=False)\n",
    "\n",
    "# initialize trainer\n",
    "ppo_config = {'eos_token_id': tokenizer.eos_token_id, 'init_kl_coef':0.9}\n",
    "ppo_trainer = PPOTrainer(model, model_ref, **ppo_config)\n",
    "\n",
    "# prepare base model inputs and outputs\n",
    "train_examples = read_examples(args.train_filename, args)\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer, args, stage='train')\n",
    "dev_examples = read_examples(args.dev_filename, args)\n",
    "dev_features = convert_examples_to_features(dev_examples, tokenizer, args, stage='train')\n",
    "test_examples = read_examples(args.test_filename, args)\n",
    "test_features = convert_examples_to_features(test_examples, tokenizer, args, stage='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085ee05",
   "metadata": {},
   "source": [
    "### Run PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcfef57f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_2572091/140047748.py:25: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  rewards = np.zeros_like(code_ids, dtype=np.float)\n",
      "# errors per sample:1.175: 100%|████████████████████████████████████████████████████████| 10/10 [02:22<00:00, 14.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 # errors 188 BLEU 63.43/83.65 nerrors 425/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# errors per sample:2.225: 100%|████████████████████████████████████████████████████████| 10/10 [02:12<00:00, 13.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 # errors 356 BLEU 42.61/83.2 nerrors 290/27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# errors per sample:1.5625: 100%|███████████████████████████████████████████████████████| 10/10 [01:27<00:00,  8.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 # errors 250 BLEU 0.0/83.33 nerrors 52/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# errors per sample:0.59375: 100%|██████████████████████████████████████████████████████| 10/10 [01:08<00:00,  6.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 # errors 95 BLEU 0.0/84.31 nerrors 112/23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# errors per sample:0.5625:   0%|                                                                | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m# errors per sample:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(epoch_nerrors\u001b[38;5;241m/\u001b[39mepoch_seen, \u001b[38;5;241m5\u001b[39m)))\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# train model with ppo\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# compute bleu\u001b[39;00m\n\u001b[1;32m    114\u001b[0m bleu, bleu_ref, nerrors, nerrors_ref \u001b[38;5;241m=\u001b[39m test(train_features, train_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/rl_code/trl/ppo.py:111\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, source_ids, source_mask, response_ids, scores)\u001b[0m\n\u001b[1;32m    109\u001b[0m         idx \u001b[38;5;241m=\u001b[39m idxs[i]\n\u001b[1;32m    110\u001b[0m         curr_len \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(response_ids\u001b[38;5;241m.\u001b[39mcpu()[idx,:])\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 111\u001b[0m         train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mcurr_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mcurr_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mcurr_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcurr_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m         all_stats\u001b[38;5;241m.\u001b[39mappend(train_stats)\n\u001b[1;32m    115\u001b[0m timing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/ppo/optimize_step\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt\n",
      "File \u001b[0;32m~/rl_code/trl/ppo.py:151\u001b[0m, in \u001b[0;36mPPOTrainer.train_minibatch\u001b[0;34m(self, logprobs, values, rewards, source_ids, source_mask, response_ids)\u001b[0m\n\u001b[1;32m    149\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_p \u001b[38;5;241m+\u001b[39m loss_v\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 151\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_stats\n",
      "File \u001b[0;32m~/.conda/envs/rl_code/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rl_code/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "from bleu import _bleu\n",
    "from reward import remove_special_tokens, tree_sitter_full_compile\n",
    "import numpy as np\n",
    "\n",
    "def get_reward(code_ids=None, tokenizer=None):\n",
    "    code_ids = np.array(code_ids.cpu())\n",
    "    eos_positions = []\n",
    "    max_len = code_ids.shape[1]\n",
    "    for id in code_ids:\n",
    "        if tokenizer.eos_token_id in id:\n",
    "            eos_positions.append((id==tokenizer.eos_token_id).argmax())\n",
    "        else:\n",
    "            eos_positions.append(max_len)\n",
    "\n",
    "    codes = [tokenizer.decode(id[:eos_pos], skip_special_tokens=True, clean_up_tokenization_spaces=False) \\\n",
    "             for id,eos_pos in zip(code_ids, eos_positions)]\n",
    "        \n",
    "    codes = [remove_special_tokens(code) for code in codes]\n",
    "    error_node_counts = [tree_sitter_full_compile(code) for code in codes]\n",
    "    num_errors = [i[0] for i in error_node_counts]\n",
    "    \n",
    "    num_nodes = [i[1] for i in error_node_counts]\n",
    "    rewards = np.zeros_like(code_ids, dtype=np.float)\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i, min(eos_positions[i],max_len-1)] = (num_nodes[i]-num_errors[i])*0.001\n",
    "#         rewards[i, min(eos_positions[i],max_len-1)] = -num_errors[i]\n",
    "    return torch.Tensor(rewards), num_errors, num_nodes\n",
    "\n",
    "\n",
    "\n",
    "# Prepare training data loader  \n",
    "train_data = get_dataset(train_features[:10*args.train_batch_size])\n",
    "train_dataloader = DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n",
    "\n",
    "def test(features, dataloader, prefix):\n",
    "    pbar = dataloader\n",
    "    pred_ids = []\n",
    "    pred_ids_ref = []\n",
    "    indices = []\n",
    "    nerrors = 0\n",
    "    nerrors_ref = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            source_ids,source_mask,target_ids,target_mask,ind = batch\n",
    "            preds = respond_to_batch(model, source_ids, source_mask, max_target_length=args.max_target_length, \\\n",
    "                                                     top_k=tokenizer.vocab_size, top_p=1.0)[:,1:]\n",
    "            nerrors += sum(get_reward(code_ids=preds, tokenizer=tokenizer)[1])\n",
    "            \n",
    "#             print (get_reward(code_ids=preds, tokenizer=tokenizer)[1])\n",
    "            \n",
    "            top_preds = list(preds.cpu().numpy())\n",
    "            \n",
    "#             codes = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) \\\n",
    "#                      for id in top_preds]\n",
    "#             print (codes)\n",
    "#             print ([tree_sitter_full_compile(remove_special_tokens(code))[0] for code in codes])\n",
    "            \n",
    "            pred_ids.extend(top_preds)\n",
    "            preds_ref = respond_to_batch(model_ref, source_ids, source_mask, max_target_length=args.max_target_length, \\\n",
    "                                                     top_k=tokenizer.vocab_size, top_p=1.0)[:,1:]\n",
    "            nerrors_ref += sum(get_reward(code_ids=preds_ref, tokenizer=tokenizer)[1])\n",
    "            top_preds = list(preds_ref.cpu().numpy())\n",
    "            pred_ids_ref.extend(top_preds)\n",
    "            indices.extend(list(ind.cpu().numpy()))\n",
    "            \n",
    "    p = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                                              for id in pred_ids]\n",
    "    p_ref = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                                              for id in pred_ids_ref]\n",
    "    with open(os.path.join(args.output_dir,prefix+\".model\"),'w') as f_model, \\\n",
    "            open(os.path.join(args.output_dir,prefix+\".gold\"),'w') as f_gold, \\\n",
    "              open(os.path.join(args.output_dir,prefix+\".model_ref\"),'w') as f_ref:\n",
    "                    for pred,ref,i in zip(p,p_ref,indices):\n",
    "                        f_model.write(pred+'\\n')\n",
    "                        f_ref.write(ref+'\\n')   \n",
    "                        f_gold.write(features[i].target+'\\n')\n",
    "                        \n",
    "    bleu=_bleu(os.path.join(args.output_dir,prefix+\".gold\"),  os.path.join(args.output_dir,prefix+\".model\"))\n",
    "    bleu_ref=_bleu(os.path.join(args.output_dir,prefix+\".gold\"),  os.path.join(args.output_dir,prefix+\".model_ref\"))\n",
    "    \n",
    "    return bleu, bleu_ref, nerrors, nerrors_ref\n",
    "                    \n",
    "\n",
    "# Run training\n",
    "for ep in range(args.train_epochs):\n",
    "    epoch_nerrors = 0\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    epoch_seen = 0\n",
    "    for batch in pbar:\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        source_ids,source_mask,target_ids,target_mask, _ = batch\n",
    "\n",
    "        # get model response\n",
    "        response_ids  = torch.clone(respond_to_batch(model, source_ids, source_mask, \\\n",
    "                                                     max_target_length=args.max_target_length, \\\n",
    "                                                     top_k=tokenizer.vocab_size, top_p=1.0).detach()[:,1:])\n",
    "        response_codes = tokenizer.batch_decode(response_ids, skip_special_tokens=True, \\\n",
    "                                                clean_up_tokenization_spaces=False)\n",
    "\n",
    "        \n",
    "        # define a reward for response\n",
    "        reward, num_errors, num_nodes = get_reward(code_ids=response_ids, tokenizer=tokenizer)\n",
    "        epoch_nerrors += sum(num_errors)\n",
    "        epoch_seen += len(source_ids)\n",
    "        pbar.set_description('# errors per sample:'+str(round(epoch_nerrors/epoch_seen, 5)))\n",
    "\n",
    "        # train model with ppo\n",
    "        train_stats = ppo_trainer.step(source_ids, source_mask, response_ids, reward.to(args.device))\n",
    "        \n",
    "    # compute bleu\n",
    "    bleu, bleu_ref, nerrors, nerrors_ref = test(train_features, train_dataloader, 'train')\n",
    "        \n",
    "    print ('epoch', ep, '# errors', epoch_nerrors, 'BLEU', str(round(bleu,2))+'/'+str(round(bleu_ref,2)), \\\n",
    "          'nerrors', str(nerrors)+'/'+str(nerrors_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b19386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba3216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ab812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120b2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_code",
   "language": "python",
   "name": "rl_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
